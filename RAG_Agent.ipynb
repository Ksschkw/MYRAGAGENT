{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15a9d03-6247-4a14-b85f-928660b8118b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from swarmauri.standard.documents.concrete.Document import Document\n",
    "from swarmauri.standard.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554f6873-38f8-4c05-8b30-7dbd5f541e4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = TfidfVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd43a91-2b12-41f1-a417-fadba95aa104",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(content=\"He writee Python, a little JavaScript, a little rust, i use dockerfile too\"),\n",
    "    Document(content=\"He uses sqlite for my database(small projects), json(not really a database) \"),\n",
    "    Document(content=\"He uses Render, Railway, Northflank and more for deployment\"),\n",
    "    Document(content=\"He uses docker for containerization\"),\n",
    "    Document(content=\"His course of study is software engineering\"),\n",
    "    Document(content=\"He is a Nigerian, currently in Lagos\"),\n",
    "    Document(content=\"He is from earth, i am not a martian\"),\n",
    "    Document(content=\"His name is kosisochukwu\")\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09cca0e-8b32-4833-892c-393c77ea63a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add documents to the vector_store\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b5ee2b-6e49-4ef9-917d-71824b65a7c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 documents added to the vector store\n"
     ]
    }
   ],
   "source": [
    "# Verify that they have been added\n",
    "print(f\"{len(vector_store.documents)} documents added to the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b18d8a-2d30-4cb2-b40c-e3c590bde2ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documente in te vector store: \n",
      "He writee Python, a little JavaScript, a little rust, i use dockerfile too\n",
      "He uses sqlite for my database(small projects), json(not really a database) \n",
      "He uses Render, Railway, Northflank and more for deployment\n",
      "He uses docker for containerization\n",
      "His course of study is software engineering\n",
      "He is a Nigerian, currently in Lagos\n",
      "He is from earth, i am not a martian\n",
      "His name is kosisochukwu\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print all documents\n",
    "all_docs = vector_store.get_all_documents()\n",
    "\n",
    "print(\"All documente in te vector store: \")\n",
    "for doc in all_docs:\n",
    "    print(doc.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df3dd-6b8a-4b16-a6d2-ace25453d9a2",
   "metadata": {},
   "source": [
    "# Configuring the conversation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f17b79-6e37-48cb-845f-f6ac2e88c2a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from swarmauri.standard.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.standard.messages.concrete.HumanMessage import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bb9a43d-caab-418d-9e7b-0b4217cbeb61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a system message\n",
    "system_context = SystemMessage(content=\"Your name is Kosisochukwu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42804e06-2d46-4f35-99f4-b2be8b00da75",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the conversation\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ac77582-7c6e-4050-8d4c-91b59bb9dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user message\n",
    "user_message = HumanMessage(content=\"What is my name?\")\n",
    "conversation.add_message(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d561f991-1003-4079-a4ed-95a0ba1fd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current conversation history\n",
      "Your name is Kosisochukwu\n",
      "What is my name?\n"
     ]
    }
   ],
   "source": [
    "# Print the current conversation context\n",
    "print(\"current conversation history\")\n",
    "for message in conversation.history:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7b104-29e0-4a0f-b8be-e62497146ecd",
   "metadata": {},
   "source": [
    "## Integrating the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "274f844f-3f45-4144-b2ff-7cd06d9009b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from swarmauri.standard.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.standard.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load envienvironment vaariables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5af0599f-3cf7-442c-ad27-9460a78ce416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is set g\n"
     ]
    }
   ],
   "source": [
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# check if API key is set \n",
    "if not API_KEY:\n",
    "    print(\"Please set the API key in the .env g\")\n",
    "else:\n",
    "    print(\"API key is set g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4af711-9299-46c6-8d31-854339dd3eda",
   "metadata": {},
   "source": [
    "## Function to Get Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d65853e-cb7b-4c88-92cf-66a0e9cb138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gett allowed models filtering out the ones that fail\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms=[\n",
    "        \"llama3-70b-8192\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"llava-v1.5-7b-4096-preview\",\n",
    "        \"llama-guard-3-8b\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9667d4b-1703-489a-9680-defb45598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GroqModel\n",
    "llm = LLM(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cfe09a5-573e-40db-9b10-983cb6055897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: LLM\n",
      "Type: GroqModel\n",
      "Default Name: gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# Print model information\n",
    "print(f\"Resource: {llm.resource}\")\n",
    "print(f\"Type: {llm.type}\")\n",
    "print(f\"Default Name: {llm.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffe8b590-34f0-48f7-bc2a-ec7e5566f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed Models:  ['llama3-8b-8192', 'gemma-7b-it']\n"
     ]
    }
   ],
   "source": [
    "# Get allowed models\n",
    "allowed_models = get_allowed_models(llm)\n",
    "print(\"Allowed Models: \", allowed_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dfec12a-1f32-4a76-befd-552cf37dfd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: llama3-8b-8192\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE WITH NO SYSTEM CONTEXT \n",
    "\n",
    "# Set the model name to the first available allowed model\n",
    "llm.name = allowed_models[0]\n",
    "print(f\"Using model: {llm.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe9fd410-9a79-4e46-bc58-d09f287c07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation\n",
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6463ae31-73ab-45bd-a501-4629c22ade21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a human message\n",
    "input_data = \"Hello\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75bd2048-af65-47f8-a2ba-8e61ec2bb57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with no system context for llama3-8b-8192: Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction =  conversation.get_last().content\n",
    "print(f\"Prediction with no system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f536d70-c481-44c1-a885-07b404d0094f",
   "metadata": {},
   "source": [
    "## Example Usage with a System Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d62f3604-4ec0-4b38-9733-65a565744be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context =  \"You are an assistant that proides answers to the user.\"\n",
    "conversation = MaxSystemContextConversation(system_context=SystemMessage(content=system_context), max_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85402c99-d89b-45fe-9fb7-098a93a86696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a human message with the content \"HI\" and add it to the conversation\n",
    "human_message = HumanMessage(content=\"Hi\")\n",
    "conversation.add_message(human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8683204a-33f3-49f0-acd1-e9a35acfc2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with system context for llama3-8b-8192: Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# Predict response\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction with system context for {llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada281f-2667-4ca8-a2cc-c5e56141ca6c",
   "metadata": {},
   "source": [
    "# BUILDING THE RAG AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82a312f7-1939-482e-9020-a20032471e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.standard.agents.concrete.RagAgent import RagAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2a71546-e909-4df7-a1de-f7a15370f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a new system context for the rag agent\n",
    "rag_system_context = \"You are an assistant that provides answers to the user. You utilize the details below: \"\n",
    "# Create a new conversation for RAG Agent\n",
    "rag_conversation = MaxSystemContextConversation(system_context=SystemMessage(content=rag_system_context), max_size=4)\n",
    "\n",
    "# Initialize the RAG Agent by combining LLM, convrsation, and vector store\n",
    "rag_agent = RagAgent(\n",
    "    llm=llm,\n",
    "    conversation=rag_conversation,\n",
    "    system_context=rag_system_context,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6f6eb-c059-40a2-8a96-219b95e3a09b",
   "metadata": {},
   "source": [
    "## HANDLING QUERIES WITH THE RAG AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35898634-f448-4910-a48a-dddf97be7192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What languages does he write?\n",
      "RAG AgentResponse: According to the details, Kosisochukwu writes Python, a little JavaScript, and a little Rust.\n",
      "\n",
      "Query: What dataabase do they use?\n",
      "RAG AgentResponse: According to the details, Kosisochukwu uses SQLite for his small projects, and also mentions using JSON, but not really as a database.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with different queries\n",
    "queries = [\n",
    "    \"What languages does he write?\",\n",
    "    \"What dataabase do they use?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = rag_agent.exec(query)\n",
    "    print(f\"Query: {query}\\nRAG AgentResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e597ab-b61a-4ca9-9889-fe20491edc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
